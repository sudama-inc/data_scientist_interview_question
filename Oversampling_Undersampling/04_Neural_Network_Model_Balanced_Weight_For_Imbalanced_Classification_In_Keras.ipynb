{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Resources"
      ],
      "metadata": {
        "id": "R5iKJf2cAKMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* This notebook is for the blog post [Neural Network Model Balanced Weight For Imbalanced Classification In Keras\n",
        "](https://medium.com/grabngoinfo/neural-network-model-balanced-weight-for-imbalanced-classification-in-keras-68d7b6c1462c)\n",
        "* Video tutorial on [YouTube](https://www.youtube.com/watch?v=Rzv908T-S7k&list=PLVppujud2yJo0qnXjWVAa8h7fxbFJHtfJ&index=4)\n",
        "* More video tutorials on [imbalanced modeling and anomaly detection](https://www.youtube.com/playlist?list=PLVppujud2yJo0qnXjWVAa8h7fxbFJHtfJ)\n",
        "* More blog posts on [imbalanced modeling and anomaly detection](https://medium.com/@AmyGrabNGoInfo/list/databricks-and-pyspark-7b59768e202d)\n",
        "\n",
        "\n",
        "For more information about data science and machine learning, please check out myÂ [YouTube channel](https://www.youtube.com/channel/UCmbA7XB6Wb7bLwJw9ARPcYg), [Medium Page](https://medium.com/@AmyGrabNGoInfo) and [GrabNGoInfo.com](https://grabngoinfo.com/tutorials/), or follow GrabNGoInfo on [LinkedIn](https://www.linkedin.com/company/grabngoinfo/)."
      ],
      "metadata": {
        "id": "A76-w5VyzXLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro"
      ],
      "metadata": {
        "id": "OTV9OlblzCM4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Euuzgk7YMtvO"
      },
      "source": [
        "When using a neural network model to classify imbalanced data, we can adjust the balanced weight for the cost function to give more attention to the minority class. Python's Keras library has a built-in option called `class_weight` to help us achieve this quickly.\n",
        "\n",
        "One benefit of using the balanced weight adjustment is that we can use the imbalanced data to build the model directly without oversampling or under-sampling before training the model. To learn about oversampling and under-sampling techniques, please check my previous posts [here](https://grabngoinfo.com/four-oversampling-and-under-sampling-methods-for-imbalanced-classification-using-python/) and [here](https://grabngoinfo.com/ensemble-oversampling-and-under-sampling-for-imbalanced-classification-using-python/).\n",
        "\n",
        "In this tutorial, we will go over the following topics:\n",
        "* Baseline neural network model for imbalanced classification\n",
        "* Calculate class weight using sklearn\n",
        "* Apply class weight on a neural network model\n",
        "* Apply manual class weight on a neural network model\n",
        "\n",
        "Let's get started!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xj6sWGThVCZt"
      },
      "source": [
        "# Step 1: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y067-lBs7UUt"
      },
      "source": [
        "# Synthetic dataset\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Model and performance\n",
        "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import classification_report, roc_auc_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bRXh-wA7jWV"
      },
      "source": [
        "# Step 2: Create Imbalanced Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opHsnLUO7zUm"
      },
      "source": [
        "Using `make_classification` from the sklearn library, We created two classes with the ratio between the majority class and the minority class being 0.995:0.005. Two informative features were made as predictors. We did not include any redundant or repeated features in this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbsmrMBO7Qcn",
        "outputId": "9a6d1bab-b3aa-4409-8e63-121ab4fee680"
      },
      "source": [
        "# Create an imbalanced dataset\n",
        "X, y = make_classification(n_samples=100000, n_features=2, n_informative=2,\n",
        "                           n_redundant=0, n_repeated=0, n_classes=2,\n",
        "                           n_clusters_per_class=1,\n",
        "                           weights=[0.995, 0.005],\n",
        "                           class_sep=0.5, random_state=0)\n",
        "\n",
        "# Convert the data from numpy array to a pandas dataframe\n",
        "df = pd.DataFrame({'feature1': X[:, 0], 'feature2': X[:, 1], 'target': y})\n",
        "\n",
        "# Check the target distribution\n",
        "df['target'].value_counts(normalize = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.9897\n",
              "1    0.0103\n",
              "Name: target, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ivMOBh4GLcl"
      },
      "source": [
        "The output shows that we have about 1% of the data in the minority class and 99% in the majority class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_7OnEeUCJ0V"
      },
      "source": [
        "# Step 3: Train Test Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTUkLJC2CSns"
      },
      "source": [
        "In this step, we split the dataset into 80% training data and 20% validation data. random_state ensures that we have the same train test split every time. The seed number for random_state does not have to be 42, and it can be any number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF_EmIVnFh1n",
        "outputId": "bdb7b37d-e791-4af7-c443-f8e69f4da754"
      },
      "source": [
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the number of records\n",
        "print('The number of records in the training dataset is', X_train.shape[0])\n",
        "print('The number of records in the test dataset is', X_test.shape[0])\n",
        "print(f\"The training dataset has {sorted(Counter(y_train).items())[0][1]} records for the majority class and {sorted(Counter(y_train).items())[1][1]} records for the minority class.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of records in the training dataset is 80000\n",
            "The number of records in the test dataset is 20000\n",
            "The training dataset has 79183 records for the majority class and 817 records for the minority class.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdH5fx-oFojt"
      },
      "source": [
        "The train test split gives us 80,000 records for the training dataset and 20,000 for the validation dataset. Thus, we have 79,183 data points from the majority class and 817 from the minority class in the training dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEEc7iejkyU3"
      },
      "source": [
        "# Step 4: Baseline Neural Network Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "503k5tX2kyQH"
      },
      "source": [
        "This step creates a neural network model on the imbalanced training datasets as the baseline model.\n",
        "\n",
        "We created the neural network model with one input layer, one hidden layer, and one output layer. Since we have two features, the input_dim is 2. We set the input layer to have two neurons, the hidden layer to have two neurons, and the output layer to have one neuron.\n",
        "\n",
        "The activation function for the input and hidden layers is `'relu'`, a popular activation function with good performance. The output activation function is `'sigmoid'`, which is used for binary classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJlDadwsZTJ0"
      },
      "source": [
        "# Train the neural network model using the imbalanced dataset\n",
        "# Create model\n",
        "nn_model=Sequential()\n",
        "nn_model.add(Dense(2,input_dim=2,activation='relu'))\n",
        "nn_model.add(Dense(2,activation='relu'))\n",
        "nn_model.add(Dense(1,activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRJz7cdYZVkI"
      },
      "source": [
        "We set the loss to be `'binary_crossentropy'` when compiling the model because we are building a binary classification model. For a multi-class classification model, the loss is usually `'categorical_crossentropy'`, and for a linear regression model, the loss is usually `'mean_squared_error'`.\n",
        "\n",
        "The optimizer is responsible for changing the weights and the learning rate to reduce the loss. `'adam'` is a widely used optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEV35j-O0zbU"
      },
      "source": [
        "#Compile model\n",
        "nn_model.compile(loss='binary_crossentropy',optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOq0oAMn1EXX"
      },
      "source": [
        "After compiling the model, we fit the neural network model on the training dataset. The `epochs` of 50 mean that the model will go through the training dataset 50 times. The `batch_size` of 100 means that each time the weights are updated, 100 data points are used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ygd-iwSP2k-i",
        "outputId": "6a6c9448-86b4-4781-9f49-60e4e7c78d3b"
      },
      "source": [
        "#Fit the model\n",
        "nn_model.fit(X_train,y_train, epochs=50, batch_size=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "800/800 [==============================] - 2s 1ms/step - loss: 0.2843\n",
            "Epoch 2/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0560\n",
            "Epoch 3/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0549\n",
            "Epoch 4/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0548\n",
            "Epoch 5/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0548\n",
            "Epoch 6/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0548\n",
            "Epoch 7/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0548\n",
            "Epoch 8/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0547\n",
            "Epoch 9/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0548\n",
            "Epoch 10/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0547\n",
            "Epoch 11/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0546\n",
            "Epoch 12/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0546\n",
            "Epoch 13/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0545\n",
            "Epoch 14/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0545\n",
            "Epoch 15/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0544\n",
            "Epoch 16/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0543\n",
            "Epoch 17/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0543\n",
            "Epoch 18/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0542\n",
            "Epoch 19/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0541\n",
            "Epoch 20/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0540\n",
            "Epoch 21/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0540\n",
            "Epoch 22/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0539\n",
            "Epoch 23/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0539\n",
            "Epoch 24/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0538\n",
            "Epoch 25/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0538\n",
            "Epoch 26/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0538\n",
            "Epoch 27/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0537\n",
            "Epoch 28/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0537\n",
            "Epoch 29/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0537\n",
            "Epoch 30/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0536\n",
            "Epoch 31/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0537\n",
            "Epoch 32/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0536\n",
            "Epoch 33/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0536\n",
            "Epoch 34/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0536\n",
            "Epoch 35/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0535\n",
            "Epoch 36/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0535\n",
            "Epoch 37/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0535\n",
            "Epoch 38/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0536\n",
            "Epoch 39/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0535\n",
            "Epoch 40/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0535\n",
            "Epoch 41/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0535\n",
            "Epoch 42/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0535\n",
            "Epoch 43/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0535\n",
            "Epoch 44/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0535\n",
            "Epoch 45/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0535\n",
            "Epoch 46/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0535\n",
            "Epoch 47/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0534\n",
            "Epoch 48/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0535\n",
            "Epoch 49/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0534\n",
            "Epoch 50/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0534\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f57f07294d0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNbyIGxo2p6C"
      },
      "source": [
        "Now let's make predictions on the testing dataset and check the model performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZei-yKz6F76"
      },
      "source": [
        "# # Prediction\n",
        "# nn_model_prediction = nn_model.predict(X_test)\n",
        "# nn_model_classes = np.argmax(nn_model_prediction,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E3oSOb46Xqz",
        "outputId": "430fbca1-5e8f-4b21-bb48-17e24c310509"
      },
      "source": [
        "# nn_model_prediction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.01133478],\n",
              "       [0.00357309],\n",
              "       [0.01286834],\n",
              "       ...,\n",
              "       [0.00298467],\n",
              "       [0.00805327],\n",
              "       [0.00740239]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqsflopB6aST",
        "outputId": "05ba1268-d8d6-46a3-9d08-b73ded789181"
      },
      "source": [
        "# nn_model_classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YCRK25t4jYL",
        "outputId": "fc933712-989c-4149-c80c-633d18ab1576"
      },
      "source": [
        "# Prediction\n",
        "nn_model_prediction = nn_model.predict(X_test)\n",
        "nn_model_classes =  [1 if i>0.5 else 0 for i in nn_model_prediction]\n",
        "\n",
        "# Check the model performance\n",
        "print(classification_report(y_test, nn_model_classes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99     19787\n",
            "           1       0.00      0.00      0.00       213\n",
            "\n",
            "    accuracy                           0.99     20000\n",
            "   macro avg       0.49      0.50      0.50     20000\n",
            "weighted avg       0.98      0.99      0.98     20000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8RfU5kb6sKn"
      },
      "source": [
        "We got a recall of 0, which means that the neural network model did not predict any minority data correctly.\n",
        "\n",
        "Let's see if the balanced weight can help us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3sBVM0xpAl1",
        "outputId": "67147327-9336-445d-8839-cfbd274de965"
      },
      "source": [
        "# Check the ROC/AUC value\n",
        "print(f'The ROC/AUC value is {roc_auc_score(y_test, nn_model_classes):.3f}.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ROC/AUC value is 0.500.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeLLZ2AL-ZmQ"
      },
      "source": [
        "# Step 5: Calculate Class Weight Using Sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_4RmP7Y-gHh"
      },
      "source": [
        "`sklearn` has a built-in utility function `compute_class_weight` to calculate the class weights.  The weights are calculated using the inverse proportion of class frequencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0WHbUuL-mtJ",
        "outputId": "570a3970-2f6a-4662-e200-2888f57528e5"
      },
      "source": [
        "# Calculate weights using sklearn\n",
        "sklearn_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
        "sklearn_weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.50515894, 48.95960832])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lli7mnOf_Lxs"
      },
      "source": [
        "The computed weights from sklearn are in array format. We need to transform it into a dictionary becauseÂ `Keras`Â takes a dictionary as inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-T6ukcb_aXX",
        "outputId": "cf3b8c5e-48d5-41fb-b6b8-aad5a1573a7c"
      },
      "source": [
        "# Transform array to dictionary\n",
        "sklearn_weights = dict(enumerate(sklearn_weights))\n",
        "sklearn_weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.5051589356301226, 1: 48.959608323133416}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9hLbttQ7BKd"
      },
      "source": [
        "# Step 6: Neural Network Model With Balance Weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xwm0zXd7OWK"
      },
      "source": [
        "In this step, we keep all the hyperparameters to be the same as the baseline model. The only difference is that we set the `class_weight` hyperparameter to be `'balanced'` when fitting the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHGRC5VVlhQw",
        "outputId": "95ccff88-bfdb-429b-8d34-f229d04939e5"
      },
      "source": [
        "# Train the neural network model using the imbalanced dataset\n",
        "# Create model\n",
        "nn_model_balanced = Sequential()\n",
        "nn_model_balanced.add(Dense(2,input_dim=2,activation='relu'))\n",
        "nn_model_balanced.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "#Compile model\n",
        "nn_model_balanced.compile(loss='binary_crossentropy',optimizer='adam')\n",
        "\n",
        "#Fit the model\n",
        "nn_model_balanced.fit(X_train,y_train, epochs=50, batch_size=100, class_weight=sklearn_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6871\n",
            "Epoch 2/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6756\n",
            "Epoch 3/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6731\n",
            "Epoch 4/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6706\n",
            "Epoch 5/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6688\n",
            "Epoch 6/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6674\n",
            "Epoch 7/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6661\n",
            "Epoch 8/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6651\n",
            "Epoch 9/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6641\n",
            "Epoch 10/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6628\n",
            "Epoch 11/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6627\n",
            "Epoch 12/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6619\n",
            "Epoch 13/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6615\n",
            "Epoch 14/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6609\n",
            "Epoch 15/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6607\n",
            "Epoch 16/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6600\n",
            "Epoch 17/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6598\n",
            "Epoch 18/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6595\n",
            "Epoch 19/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6588\n",
            "Epoch 20/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6586\n",
            "Epoch 21/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6583\n",
            "Epoch 22/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6581\n",
            "Epoch 23/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6577\n",
            "Epoch 24/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6575\n",
            "Epoch 25/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6574\n",
            "Epoch 26/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6567\n",
            "Epoch 27/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6570\n",
            "Epoch 28/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6564\n",
            "Epoch 29/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6562\n",
            "Epoch 30/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6559\n",
            "Epoch 31/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6563\n",
            "Epoch 32/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6559\n",
            "Epoch 33/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6557\n",
            "Epoch 34/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6555\n",
            "Epoch 35/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6557\n",
            "Epoch 36/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6553\n",
            "Epoch 37/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6553\n",
            "Epoch 38/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6552\n",
            "Epoch 39/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6550\n",
            "Epoch 40/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6547\n",
            "Epoch 41/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6551\n",
            "Epoch 42/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6547\n",
            "Epoch 43/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6547\n",
            "Epoch 44/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6545\n",
            "Epoch 45/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6548\n",
            "Epoch 46/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6543\n",
            "Epoch 47/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6547\n",
            "Epoch 48/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6546\n",
            "Epoch 49/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6545\n",
            "Epoch 50/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6543\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f57eeef4810>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjRfhpIOAVpH",
        "outputId": "86493c83-ae31-40b3-fc1e-2c55f7d87c37"
      },
      "source": [
        "# Prediction\n",
        "nn_model_balanced_prediction = nn_model_balanced.predict(X_test)\n",
        "nn_model_balanced_classes = [1 if i>0.5 else 0 for i in nn_model_balanced_prediction]\n",
        "\n",
        "# Check the model performance\n",
        "print(classification_report(y_test, nn_model_balanced_classes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.62      0.76     19787\n",
            "           1       0.02      0.56      0.03       213\n",
            "\n",
            "    accuracy                           0.62     20000\n",
            "   macro avg       0.50      0.59      0.40     20000\n",
            "weighted avg       0.98      0.62      0.75     20000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_eRctxWYgqj"
      },
      "source": [
        "We can see that the minority recall value increased from 0 to 56%, which is a significant improvement. Note that your results can be different than mine because of the randomness with the neural network model, but the difference should be small."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riPRS1coqSPe",
        "outputId": "4591e4eb-b978-48f3-a22c-2aa68c9405aa"
      },
      "source": [
        "# Check the ROC/AUC value\n",
        "print(f'The ROC/AUC value is {roc_auc_score(y_test, nn_model_balanced_classes):.3f}.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ROC/AUC value is 0.589.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN-a0OC1Ar21"
      },
      "source": [
        "# Step 7: Manual Balance Weight On Neural Network Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyZVcdmog4OV"
      },
      "source": [
        "Although the balance weights are commonly calculated using the inverse proportion of class frequencies, we can set our own balance weight and tune it as a hyperparameter. For example, we can set the cost penalty ratio to be 1:200."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMEUJgozjoUb",
        "outputId": "6fe67f62-b213-4427-cb07-7891e59e22bc"
      },
      "source": [
        "manual_weights = {0: 1, 1: 200}\n",
        "\n",
        "# Train the neural network model using the imbalanced dataset\n",
        "# Create model\n",
        "nn_model_mbalanced = Sequential()\n",
        "nn_model_mbalanced.add(Dense(2,input_dim=2,activation='relu'))\n",
        "nn_model_mbalanced.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "#Compile model\n",
        "nn_model_mbalanced.compile(loss='binary_crossentropy',optimizer='adam')\n",
        "\n",
        "#Fit the model\n",
        "nn_model_mbalanced.fit(X_train,y_train, epochs=50, batch_size=100, class_weight=manual_weights)\n",
        "\n",
        "# Prediction\n",
        "nn_model_mbalanced_prediction = nn_model_mbalanced.predict(X_test)\n",
        "nn_model_mbalanced_classes = [1 if i>0.5 else 0 for i in nn_model_mbalanced_prediction]\n",
        "\n",
        "# Check the model performance\n",
        "print(classification_report(y_test, nn_model_mbalanced_classes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.0946\n",
            "Epoch 2/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.9795\n",
            "Epoch 3/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.9331\n",
            "Epoch 4/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.9067\n",
            "Epoch 5/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.8852\n",
            "Epoch 6/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.8724\n",
            "Epoch 7/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.8639\n",
            "Epoch 8/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.8519\n",
            "Epoch 9/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.8141\n",
            "Epoch 10/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7947\n",
            "Epoch 11/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7889\n",
            "Epoch 12/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7856\n",
            "Epoch 13/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7827\n",
            "Epoch 14/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7808\n",
            "Epoch 15/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7790\n",
            "Epoch 16/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7782\n",
            "Epoch 17/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7773\n",
            "Epoch 18/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7766\n",
            "Epoch 19/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7757\n",
            "Epoch 20/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7756\n",
            "Epoch 21/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7748\n",
            "Epoch 22/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7747\n",
            "Epoch 23/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7744\n",
            "Epoch 24/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7746\n",
            "Epoch 25/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7744\n",
            "Epoch 26/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7742\n",
            "Epoch 27/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7739\n",
            "Epoch 28/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7737\n",
            "Epoch 29/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7738\n",
            "Epoch 30/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7740\n",
            "Epoch 31/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7738\n",
            "Epoch 32/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7734\n",
            "Epoch 33/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7734\n",
            "Epoch 34/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7736\n",
            "Epoch 35/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7738\n",
            "Epoch 36/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7726\n",
            "Epoch 37/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7738\n",
            "Epoch 38/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7735\n",
            "Epoch 39/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7732\n",
            "Epoch 40/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7733\n",
            "Epoch 41/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7732\n",
            "Epoch 42/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7736\n",
            "Epoch 43/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7731\n",
            "Epoch 44/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7727\n",
            "Epoch 45/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7731\n",
            "Epoch 46/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7731\n",
            "Epoch 47/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7731\n",
            "Epoch 48/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7729\n",
            "Epoch 49/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7731\n",
            "Epoch 50/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7731\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.08      0.16     19787\n",
            "           1       0.01      0.98      0.02       213\n",
            "\n",
            "    accuracy                           0.09     20000\n",
            "   macro avg       0.50      0.53      0.09     20000\n",
            "weighted avg       0.99      0.09      0.15     20000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfuk1nwPaTjy"
      },
      "source": [
        "We are able to capture 98% of the minority class after increasing the cost penalty for the minority class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyHPTa46qazt",
        "outputId": "d6dbb3e9-6375-4a27-8f26-f25b15876313"
      },
      "source": [
        "# Check the ROC/AUC value\n",
        "print(f'The ROC/AUC value is {roc_auc_score(y_test, nn_model_mbalanced_classes):.3f}.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ROC/AUC value is 0.535.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hzr7Yr7bSIq"
      },
      "source": [
        "# Step 8: Put All Code Together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUnrSCHsAiMe"
      },
      "source": [
        "###### Step 1: Import Libraries\n",
        "\n",
        "# Synthetic dataset\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Model and performance\n",
        "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "###### Step 2: Create Imbalanced Dataset\n",
        "\n",
        "# Create an imbalanced dataset\n",
        "X, y = make_classification(n_samples=100000, n_features=2, n_informative=2,\n",
        "                           n_redundant=0, n_repeated=0, n_classes=2,\n",
        "                           n_clusters_per_class=1,\n",
        "                           weights=[0.995, 0.005],\n",
        "                           class_sep=0.5, random_state=0)\n",
        "\n",
        "# Convert the data from numpy array to a pandas dataframe\n",
        "df = pd.DataFrame({'feature1': X[:, 0], 'feature2': X[:, 1], 'target': y})\n",
        "\n",
        "# Check the target distribution\n",
        "df['target'].value_counts(normalize = True)\n",
        "\n",
        "\n",
        "###### Step 3: Train Test Split\n",
        "\n",
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the number of records\n",
        "print('The number of records in the training dataset is', X_train.shape[0])\n",
        "print('The number of records in the test dataset is', X_test.shape[0])\n",
        "print(f\"The training dataset has {sorted(Counter(y_train).items())[0][1]} records for the majority class and {sorted(Counter(y_train).items())[1][1]} records for the minority class.\")\n",
        "\n",
        "\n",
        "###### Step 4: Baseline Neural Network Model\n",
        "\n",
        "# Train the neural network model using the imbalanced dataset\n",
        "# Create model\n",
        "nn_model=Sequential()\n",
        "nn_model.add(Dense(2,input_dim=2,activation='relu'))\n",
        "nn_model.add(Dense(2,activation='relu'))\n",
        "nn_model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "#Compile model\n",
        "nn_model.compile(loss='binary_crossentropy',optimizer='adam')\n",
        "\n",
        "#Fit the model\n",
        "nn_model.fit(X_train,y_train, epochs=50, batch_size=100)\n",
        "\n",
        "# Prediction\n",
        "nn_model_prediction = nn_model.predict(X_test)\n",
        "nn_model_classes =  [1 if i>0.5 else 0 for i in nn_model_prediction]\n",
        "\n",
        "# Check the model performance\n",
        "print(classification_report(y_test, nn_model_classes))\n",
        "\n",
        "\n",
        "###### Step 5: Calcuate Class Weight Using Sklearn\n",
        "\n",
        "# Calculate weights using sklearn\n",
        "sklearn_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
        "sklearn_weights\n",
        "\n",
        "# Transform array to dictionary\n",
        "sklearn_weights = dict(enumerate(sklearn_weights))\n",
        "sklearn_weights\n",
        "\n",
        "\n",
        "###### Step 6: Neural Network Model With Balance Weight\n",
        "\n",
        "# Train the neural network model using the imbalanced dataset\n",
        "# Create model\n",
        "nn_model_balanced = Sequential()\n",
        "nn_model_balanced.add(Dense(2,input_dim=2,activation='relu'))\n",
        "nn_model_balanced.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "#Compile model\n",
        "nn_model_balanced.compile(loss='binary_crossentropy',optimizer='adam')\n",
        "\n",
        "#Fit the model\n",
        "nn_model_balanced.fit(X_train,y_train, epochs=50, batch_size=100, class_weight=sklearn_weights)\n",
        "\n",
        "# Prediction\n",
        "nn_model_balanced_prediction = nn_model_balanced.predict(X_test)\n",
        "nn_model_balanced_classes = [1 if i>0.5 else 0 for i in nn_model_balanced_prediction]\n",
        "\n",
        "# Check the model performance\n",
        "print(classification_report(y_test, nn_model_balanced_classes))\n",
        "\n",
        "\n",
        "###### Step 7: Manual Balance Weight On Neural Network Model\n",
        "\n",
        "manual_weights = {0: 1, 1: 200}\n",
        "\n",
        "# Train the neural network model using the imbalanced dataset\n",
        "# Create model\n",
        "nn_model_mbalanced = Sequential()\n",
        "nn_model_mbalanced.add(Dense(2,input_dim=2,activation='relu'))\n",
        "nn_model_mbalanced.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "#Compile model\n",
        "nn_model_mbalanced.compile(loss='binary_crossentropy',optimizer='adam')\n",
        "\n",
        "#Fit the model\n",
        "nn_model_mbalanced.fit(X_train,y_train, epochs=50, batch_size=100, class_weight=manual_weights)\n",
        "\n",
        "# Prediction\n",
        "nn_model_mbalanced_prediction = nn_model_mbalanced.predict(X_test)\n",
        "nn_model_mbalanced_classes = [1 if i>0.5 else 0 for i in nn_model_mbalanced_prediction]\n",
        "\n",
        "# Check the model performance\n",
        "print(classification_report(y_test, nn_model_mbalanced_classes))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ajk8FMzDLSH"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T25kZwBkDa_t"
      },
      "source": [
        "We built the neural network models with and without the balanced weight for imbalanced classification in this tutorial. Results show that the balanced weight significantly improved the model's ability to capture the minority class. Python's `sklearn` library can compute the balance weight based on the frequency of minority and majority class, but we can use our own weight and adjust it as a hyperparameter."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recommended tutorials"
      ],
      "metadata": {
        "id": "QhCG5O2v02rk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [GrabNGoInfo Machine Learning Tutorials Inventory](https://medium.com/grabngoinfo/grabngoinfo-machine-learning-tutorials-inventory-9b9d78ebdd67)\n",
        "- [Hierarchical Topic Model for Airbnb Reviews](https://medium.com/p/hierarchical-topic-model-for-airbnb-reviews-f772eaa30434)\n",
        "- [3 Ways for Multiple Time Series Forecasting Using Prophet in Python](https://medium.com/p/3-ways-for-multiple-time-series-forecasting-using-prophet-in-python-7a0709a117f9)\n",
        "- [Time Series Anomaly Detection Using Prophet in Python](https://medium.com/grabngoinfo/time-series-anomaly-detection-using-prophet-in-python-877d2b7b14b4)\n",
        "- [Time Series Causal Impact Analysis in Python](https://medium.com/grabngoinfo/time-series-causal-impact-analysis-in-python-63eacb1df5cc)\n",
        "- [Hyperparameter Tuning For XGBoost](https://medium.com/p/hyperparameter-tuning-for-xgboost-91449869c57e)\n",
        "- [Four Oversampling And Under-Sampling Methods For Imbalanced Classification Using Python](https://medium.com/p/four-oversampling-and-under-sampling-methods-for-imbalanced-classification-using-python-7304aedf9037)\n",
        "- [Five Ways To Create Tables In Databricks](https://medium.com/grabngoinfo/five-ways-to-create-tables-in-databricks-cd3847cfc3aa)\n",
        "- [Explainable S-Learner Uplift Model Using Python Package CausalML](https://medium.com/grabngoinfo/explainable-s-learner-uplift-model-using-python-package-causalml-a3c2bed3497c)\n",
        "- [One-Class SVM For Anomaly Detection](https://medium.com/p/one-class-svm-for-anomaly-detection-6c97fdd6d8af)\n",
        "- [Recommendation System: Item-Based Collaborative Filtering](https://medium.com/grabngoinfo/recommendation-system-item-based-collaborative-filtering-f5078504996a)\n",
        "- [Hyperparameter Tuning for Time Series Causal Impact Analysis in Python](https://medium.com/grabngoinfo/hyperparameter-tuning-for-time-series-causal-impact-analysis-in-python-c8f7246c4d22)\n",
        "- [Hyperparameter Tuning and Regularization for Time Series Model Using Prophet in Python](https://medium.com/grabngoinfo/hyperparameter-tuning-and-regularization-for-time-series-model-using-prophet-in-python-9791370a07dc)\n",
        "- [Multivariate Time Series Forecasting with Seasonality and Holiday Effect Using Prophet in Python](https://medium.com/p/multivariate-time-series-forecasting-with-seasonality-and-holiday-effect-using-prophet-in-python-d5d4150eeb57)\n",
        "- [LASSO (L1) Vs Ridge (L2) Vs Elastic Net Regularization For Classification Model](https://medium.com/towards-artificial-intelligence/lasso-l1-vs-ridge-l2-vs-elastic-net-regularization-for-classification-model-409c3d86f6e9)\n",
        "- [S Learner Uplift Model for Individual Treatment Effect and Customer Segmentation in Python](https://medium.com/grabngoinfo/s-learner-uplift-model-for-individual-treatment-effect-and-customer-segmentation-in-python-9d410746e122)\n",
        "- [How to Use R with Google Colab Notebook](https://medium.com/p/how-to-use-r-with-google-colab-notebook-610c3a2f0eab)"
      ],
      "metadata": {
        "id": "ymDETcv31HAW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f7b-RE4kx-S"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Unyy7rwU2veT"
      },
      "source": [
        "* [keras documentation](https://keras.io/api/)\n"
      ]
    }
  ]
}