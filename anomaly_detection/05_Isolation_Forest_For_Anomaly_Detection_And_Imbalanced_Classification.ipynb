{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Resources"
      ],
      "metadata": {
        "id": "fLsRdRYnAQlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* This notebook is for the blog post [Isolation Forest For Anomaly Detection\n",
        "](https://medium.com/grabngoinfo/isolation-forest-for-anomaly-detection-cd7871ae99b4)\n",
        "* Video tutorial on [YouTube](https://www.youtube.com/watch?v=DoUhCLbLeH0&list=PLVppujud2yJo0qnXjWVAa8h7fxbFJHtfJ&index=5)\n",
        "* More video tutorials on [imbalanced modeling and anomaly detection](https://www.youtube.com/playlist?list=PLVppujud2yJo0qnXjWVAa8h7fxbFJHtfJ)\n",
        "* More blog posts on [imbalanced modeling and anomaly detection](https://medium.com/@AmyGrabNGoInfo/list/databricks-and-pyspark-7b59768e202d)\n",
        "\n",
        "\n",
        "For more information about data science and machine learning, please check out myÂ [YouTube channel](https://www.youtube.com/channel/UCmbA7XB6Wb7bLwJw9ARPcYg), [Medium Page](https://medium.com/@AmyGrabNGoInfo) and [GrabNGoInfo.com](https://grabngoinfo.com/tutorials/), or follow GrabNGoInfo on [LinkedIn](https://www.linkedin.com/company/grabngoinfo/)."
      ],
      "metadata": {
        "id": "xCgUws151tFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro"
      ],
      "metadata": {
        "id": "sGFvOtVL1lUQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ifIKL5HuJ6f"
      },
      "source": [
        "Isolation forest uses the number of tree splits to identify anomalies or minority classes in an imbalanced dataset. The idea is that anomaly data points take fewer splits because the density around the anomalies is low. Python's sklearn library has an implementation for the isolation forest model.\n",
        "\n",
        "Isolation forest is an unsupervised algorithm, where the actual labels of normal vs. anomaly data points are not used in model training.\n",
        "\n",
        "To learn how to use supervised models to identify abnormal data points, please refer to [Four Oversampling And Under-Sampling Methods For Imbalanced Classification Using Python](https://grabngoinfo.com/four-oversampling-and-under-sampling-methods-for-imbalanced-classification-using-python/), and [Neural Network Model Balanced Weight For Imbalanced Classification In Keras](https://grabngoinfo.com/neural-network-model-balanced-weight-for-imbalanced-classification-in-keras/).\n",
        "\n",
        "In this article, you will learn\n",
        "* What is the isolation forest model\n",
        "* How to build an isolation forest model using Python\n",
        "* How to use an isolation forest model to do anomaly detection\n",
        "* How to continue training an isolation forest model using new data\n",
        "* How to continue training an isolation forest model using more trees\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xj6sWGThVCZt"
      },
      "source": [
        "# Step 1: Import Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnLebaDFM3jv"
      },
      "source": [
        "The first step is to import libraries. We need to import `make_classification` from `sklearn` to create the modeling dataset. Import `pandas` and `numpy` for data processing, `Counter` will help us count the number of records.\n",
        "\n",
        "We also need the `train_test_split` to create training and validation dataset. `IsolationForest` for modeling, and `classification_report` for model performance evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y067-lBs7UUt"
      },
      "source": [
        "# Synthetic dataset\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Model and performance\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import classification_report\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bRXh-wA7jWV"
      },
      "source": [
        "# Step 2: Create Imbalanced Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opHsnLUO7zUm"
      },
      "source": [
        "Using `make_classification` from the `sklearn` library, We created two classes with the ratio between the majority class and the minority class being 0.995:0.005. Two informative features were made as predictors. We did not include any redundant or repeated features in this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbsmrMBO7Qcn",
        "outputId": "784b7a54-d848-4fef-90e0-68e4eed91584"
      },
      "source": [
        "# Create an imbalanced dataset\n",
        "X, y = make_classification(n_samples=100000, n_features=2, n_informative=2,\n",
        "                           n_redundant=0, n_repeated=0, n_classes=2,\n",
        "                           n_clusters_per_class=1,\n",
        "                           weights=[0.995, 0.005],\n",
        "                           class_sep=0.5, random_state=0)\n",
        "\n",
        "# Convert the data from numpy array to a pandas dataframe\n",
        "df = pd.DataFrame({'feature1': X[:, 0], 'feature2': X[:, 1], 'target': y})\n",
        "\n",
        "# Check the target distribution\n",
        "df['target'].value_counts(normalize = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.9897\n",
              "1    0.0103\n",
              "Name: target, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ivMOBh4GLcl"
      },
      "source": [
        "The output shows that we have about 1% of the data in the minority class and 99% in the majority class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_7OnEeUCJ0V"
      },
      "source": [
        "# Step 3: Train Test Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTUkLJC2CSns"
      },
      "source": [
        "In this step, we split the dataset into 80% training data and 20% validation data. random_state ensures that we have the same train test split every time. The seed number for random_state does not have to be 42, and it can be any number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF_EmIVnFh1n",
        "outputId": "f9b66904-3848-424b-9851-b21bba09d726"
      },
      "source": [
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the number of records\n",
        "print('The number of records in the training dataset is', X_train.shape[0])\n",
        "print('The number of records in the test dataset is', X_test.shape[0])\n",
        "print(f\"The training dataset has {sorted(Counter(y_train).items())[0][1]} records for the majority class and {sorted(Counter(y_train).items())[1][1]} records for the minority class.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of records in the training dataset is 80000\n",
            "The number of records in the test dataset is 20000\n",
            "The training dataset has 79183 records for the majority class and 817 records for the minority class.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdH5fx-oFojt"
      },
      "source": [
        "The train test split gives us 80,000 records for the training dataset and 20,000 for the validation dataset. Thus, we have 79,183 data points from the majority class and 817 from the minority class in the training dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvG0hEQ4aAUp"
      },
      "source": [
        "# Step 4: Train Isolation Forest Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mnjHJEdaTFN"
      },
      "source": [
        "Isolation forest identify anomalies by isolating outliers using trees. The steps are:\n",
        "1. For a tree, randomly select features and randomly split for each feature.\n",
        "2. For each data point, there is a splitting path from the root node to the leaf node. Calculate the path length for each data point.\n",
        "3. Repeat step 1 and step 2 for each tree.\n",
        "4. Get the average path length across all trees.\n",
        "5. The anomalies have a shorter average path length than normal data points.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKOs7thlOynG",
        "outputId": "6037d62f-4cd6-4ed7-de68-b0cb371505f1"
      },
      "source": [
        "# Train the isolation forest model\n",
        "if_model = IsolationForest(n_estimators=100, random_state=0).fit(X_train)\n",
        "\n",
        "# Predict the anomalies\n",
        "if_prediction = if_model.predict(X_test)\n",
        "\n",
        "# Change the anomalies' values to make it consistent with the true values\n",
        "if_prediction = [1 if i==-1 else 0 for i in if_prediction]\n",
        "\n",
        "# Check the model performance\n",
        "print(classification_report(y_test, if_prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.79      0.88     19787\n",
            "           1       0.02      0.38      0.04       213\n",
            "\n",
            "    accuracy                           0.79     20000\n",
            "   macro avg       0.51      0.58      0.46     20000\n",
            "weighted avg       0.98      0.79      0.87     20000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe3rxiW_TMTz"
      },
      "source": [
        "We train the isolation forest model using the training dataset and make the predictions on the testing dataset. By default, isolation forest labels the normal data points as 1s and anomalies as -1s. To compare the labels with the ground truth in the testing dataset, we changed the anomalies' labels from -1 to 1, and the normal labels from 1 to 0.\n",
        "\n",
        "The model has a recall values of 38%, meaning that it captures 38% of the anomaly data points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6Kd8ISRUvkc"
      },
      "source": [
        "# Step 5: Isolation Forest With Warm Start On New Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cafwS0f6U2vM"
      },
      "source": [
        "Isolation forest supports a warm start, which can train trees in addition to the existing model.\n",
        "\n",
        "Suppose we collected more data after training the model. Then, we can utilize the new data collected and train on top of the existing model.\n",
        "\n",
        "Let's create more data using `make_classification`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARTBRUGWayVP"
      },
      "source": [
        "# Create more imbalanced data\n",
        "X_more, y_more = make_classification(n_samples=5000, n_features=2, n_informative=2,\n",
        "                           n_redundant=0, n_repeated=0, n_classes=2,\n",
        "                           n_clusters_per_class=1,\n",
        "                           weights=[0.995, 0.005],\n",
        "                           class_sep=0.5, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0hb-oUDbJkl",
        "outputId": "77a3cb5b-3305-45aa-bad5-d383cc509bba"
      },
      "source": [
        "# # Train the isolation forest model\n",
        "# if_model = IsolationForest(n_estimators=100, random_state=0, warm_start=True).fit(X_train)\n",
        "\n",
        "# # Use new data to train 50 trees on top of existing model\n",
        "# if_model.set_params(n_estimators=150, random_state=0).fit(X_more)\n",
        "\n",
        "# # Predict the anomalies\n",
        "# if_prediction = if_model.predict(X_test)\n",
        "\n",
        "# # Change the anomalies' values from -1 to 0\n",
        "# if_prediction = [0 if i==-1 else i for i in if_prediction]\n",
        "\n",
        "# # Check the model performance\n",
        "# print(classification_report(y_test, if_prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.26      0.41     19787\n",
            "           1       0.01      0.61      0.02       213\n",
            "\n",
            "    accuracy                           0.26     20000\n",
            "   macro avg       0.50      0.43      0.21     20000\n",
            "weighted avg       0.97      0.26      0.41     20000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3cOHJoogZkk"
      },
      "source": [
        "We set the option of `warm_start=True` for the original isolation forest model, then added 50 trees trained on the new dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ul1WzWWOfJVy",
        "outputId": "ec9d240b-21ff-4d2a-f7a8-1b43a4a5cbe0"
      },
      "source": [
        "# Train the isolation forest model\n",
        "if_model = IsolationForest(n_estimators=100, random_state=0, warm_start=True).fit(X_train)\n",
        "\n",
        "# Use new data to train 50 trees on top of existing model\n",
        "if_model.n_estimators += 50\n",
        "if_model.fit(X_more)\n",
        "\n",
        "# Predict the anomalies\n",
        "if_prediction = if_model.predict(X_test)\n",
        "\n",
        "# Change the anomalies' values to make it consistent with the true values\n",
        "if_prediction = [1 if i==-1 else 0 for i in if_prediction]\n",
        "\n",
        "# Check the model performance\n",
        "print(classification_report(y_test, if_prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.74      0.85     19787\n",
            "           1       0.02      0.39      0.03       213\n",
            "\n",
            "    accuracy                           0.74     20000\n",
            "   macro avg       0.50      0.57      0.44     20000\n",
            "weighted avg       0.98      0.74      0.84     20000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ulr6xsaKGzA"
      },
      "source": [
        "We see a 1% increase in the recall value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-uc3KoExJm9"
      },
      "source": [
        "# Step 6: Isolation Forest With Warm Start On New Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo7dxddkxZUA"
      },
      "source": [
        "Even when no new data is available, we can still train the isolation forest with a warm start to improve model performance by introducing more trees.\n",
        "\n",
        "The code below shows how to train additional trees using the same modeling dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZXibcNyyprH",
        "outputId": "05fccc9e-9f4e-4dc3-e198-6db092884181"
      },
      "source": [
        "# Train the isolation forest model\n",
        "if_model = IsolationForest(n_estimators=100, random_state=0, warm_start=True).fit(X_train)\n",
        "\n",
        "# Use the existing data to train 20 trees on top of existing model\n",
        "if_model.n_estimators += 20\n",
        "if_model.fit(X_train)\n",
        "\n",
        "# Predict the anomalies\n",
        "if_prediction = if_model.predict(X_test)\n",
        "\n",
        "# Change the anomalies' values to make it consistent with the true values\n",
        "if_prediction = [1 if i==-1 else 0 for i in if_prediction]\n",
        "\n",
        "# Check the model performance\n",
        "print(classification_report(y_test, if_prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.79      0.88     19787\n",
            "           1       0.02      0.38      0.04       213\n",
            "\n",
            "    accuracy                           0.79     20000\n",
            "   macro avg       0.51      0.58      0.46     20000\n",
            "weighted avg       0.98      0.79      0.87     20000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPJxgzzt1FKU"
      },
      "source": [
        "We can choose to keep training the isolation model using more data, more trees on existing data, or use both."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ggv-Nn_zfZ4"
      },
      "source": [
        "# Step 7: Put All Code Together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPi1H-kbzuVU"
      },
      "source": [
        "###### Step 1: Import Libraries\n",
        "\n",
        "# Synthetic dataset\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Model and performance\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "###### Step 2: Create Imbalanced Dataset\n",
        "\n",
        "# Create an imbalanced dataset\n",
        "X, y = make_classification(n_samples=100000, n_features=2, n_informative=2,\n",
        "                           n_redundant=0, n_repeated=0, n_classes=2,\n",
        "                           n_clusters_per_class=1,\n",
        "                           weights=[0.995, 0.005],\n",
        "                           class_sep=0.5, random_state=0)\n",
        "\n",
        "# Convert the data from numpy array to a pandas dataframe\n",
        "df = pd.DataFrame({'feature1': X[:, 0], 'feature2': X[:, 1], 'target': y})\n",
        "\n",
        "# Check the target distribution\n",
        "df['target'].value_counts(normalize = True)\n",
        "\n",
        "\n",
        "###### Step 3: Train Test Split\n",
        "\n",
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the number of records\n",
        "print('The number of records in the training dataset is', X_train.shape[0])\n",
        "print('The number of records in the test dataset is', X_test.shape[0])\n",
        "print(f\"The training dataset has {sorted(Counter(y_train).items())[0][1]} records for the majority class and {sorted(Counter(y_train).items())[1][1]} records for the minority class.\")\n",
        "\n",
        "\n",
        "###### Step 4: Train Isolation Forest Model\n",
        "\n",
        "# Train the isolation forest model\n",
        "if_model = IsolationForest(n_estimators=100, random_state=0).fit(X_train)\n",
        "\n",
        "# Predict the anomalies\n",
        "if_prediction = if_model.predict(X_test)\n",
        "\n",
        "# Change the anomalies' values to make it consistent with the true values\n",
        "if_prediction = [1 if i==-1 else 0 for i in if_prediction]\n",
        "\n",
        "# Check the model performance\n",
        "print(classification_report(y_test, if_prediction))\n",
        "\n",
        "\n",
        "###### Step 5: Isolation Forest With Warm Start On New Data\n",
        "\n",
        "# Create more imbalanced data\n",
        "X_more, y_more = make_classification(n_samples=5000, n_features=2, n_informative=2,\n",
        "                           n_redundant=0, n_repeated=0, n_classes=2,\n",
        "                           n_clusters_per_class=1,\n",
        "                           weights=[0.995, 0.005],\n",
        "                           class_sep=0.5, random_state=0)\n",
        "\n",
        "# Train the isolation forest model\n",
        "if_model = IsolationForest(n_estimators=100, random_state=0, warm_start=True).fit(X_train)\n",
        "\n",
        "# Use new data to train 50 trees on top of existing model\n",
        "if_model.n_estimators += 50\n",
        "if_model.fit(X_more)\n",
        "\n",
        "# Predict the anomalies\n",
        "if_prediction = if_model.predict(X_test)\n",
        "\n",
        "# Change the anomalies' values to make it consistent with the true values\n",
        "if_prediction = [1 if i==-1 else 0 for i in if_prediction]\n",
        "\n",
        "# Check the model performance\n",
        "print(classification_report(y_test, if_prediction))\n",
        "\n",
        "\n",
        "###### Step 6: Isolation Forest With Warm Start On New Trees\n",
        "\n",
        "# Train the isolation forest model\n",
        "if_model = IsolationForest(n_estimators=100, random_state=0, warm_start=True).fit(X_train)\n",
        "\n",
        "# Use the existing data to train 20 trees on top of existing model\n",
        "if_model.n_estimators += 20\n",
        "if_model.fit(X_train)\n",
        "\n",
        "# Predict the anomalies\n",
        "if_prediction = if_model.predict(X_test)\n",
        "\n",
        "# Change the anomalies' values to make it consistent with the true values\n",
        "if_prediction = [1 if i==-1 else 0 for i in if_prediction]\n",
        "\n",
        "# Check the model performance\n",
        "print(classification_report(y_test, if_prediction))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04lSJpzi1e-V"
      },
      "source": [
        "# Step 8: Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpYUsNLT1qv5"
      },
      "source": [
        "In this article, we created a synthetic dataset with anomalies and used it to go through using isolation forest to make anomaly detection.\n",
        "\n",
        "Using the `sklearn` library in Python, we covered\n",
        "* What is the isolation forest model\n",
        "* How to build an isolation forest model using Python\n",
        "* How to use an isolation forest model to do anomaly detection\n",
        "* How to continue training an isolation forest model using new data\n",
        "* How to continue training an isolation forest model using more trees\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recommended Tutorials"
      ],
      "metadata": {
        "id": "MHEIUWaL2ZIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [GrabNGoInfo Machine Learning Tutorials Inventory](https://medium.com/grabngoinfo/grabngoinfo-machine-learning-tutorials-inventory-9b9d78ebdd67)\n",
        "- [Hierarchical Topic Model for Airbnb Reviews](https://medium.com/p/hierarchical-topic-model-for-airbnb-reviews-f772eaa30434)\n",
        "- [3 Ways for Multiple Time Series Forecasting Using Prophet in Python](https://medium.com/p/3-ways-for-multiple-time-series-forecasting-using-prophet-in-python-7a0709a117f9)\n",
        "- [Time Series Anomaly Detection Using Prophet in Python](https://medium.com/grabngoinfo/time-series-anomaly-detection-using-prophet-in-python-877d2b7b14b4)\n",
        "- [Time Series Causal Impact Analysis in Python](https://medium.com/grabngoinfo/time-series-causal-impact-analysis-in-python-63eacb1df5cc)\n",
        "- [Hyperparameter Tuning For XGBoost](https://medium.com/p/hyperparameter-tuning-for-xgboost-91449869c57e)\n",
        "- [Four Oversampling And Under-Sampling Methods For Imbalanced Classification Using Python](https://medium.com/p/four-oversampling-and-under-sampling-methods-for-imbalanced-classification-using-python-7304aedf9037)\n",
        "- [Five Ways To Create Tables In Databricks](https://medium.com/grabngoinfo/five-ways-to-create-tables-in-databricks-cd3847cfc3aa)\n",
        "- [Explainable S-Learner Uplift Model Using Python Package CausalML](https://medium.com/grabngoinfo/explainable-s-learner-uplift-model-using-python-package-causalml-a3c2bed3497c)\n",
        "- [One-Class SVM For Anomaly Detection](https://medium.com/p/one-class-svm-for-anomaly-detection-6c97fdd6d8af)\n",
        "- [Recommendation System: Item-Based Collaborative Filtering](https://medium.com/grabngoinfo/recommendation-system-item-based-collaborative-filtering-f5078504996a)\n",
        "- [Hyperparameter Tuning for Time Series Causal Impact Analysis in Python](https://medium.com/grabngoinfo/hyperparameter-tuning-for-time-series-causal-impact-analysis-in-python-c8f7246c4d22)\n",
        "- [Hyperparameter Tuning and Regularization for Time Series Model Using Prophet in Python](https://medium.com/grabngoinfo/hyperparameter-tuning-and-regularization-for-time-series-model-using-prophet-in-python-9791370a07dc)\n",
        "- [Multivariate Time Series Forecasting with Seasonality and Holiday Effect Using Prophet in Python](https://medium.com/p/multivariate-time-series-forecasting-with-seasonality-and-holiday-effect-using-prophet-in-python-d5d4150eeb57)\n",
        "- [LASSO (L1) Vs Ridge (L2) Vs Elastic Net Regularization For Classification Model](https://medium.com/towards-artificial-intelligence/lasso-l1-vs-ridge-l2-vs-elastic-net-regularization-for-classification-model-409c3d86f6e9)\n",
        "- [S Learner Uplift Model for Individual Treatment Effect and Customer Segmentation in Python](https://medium.com/grabngoinfo/s-learner-uplift-model-for-individual-treatment-effect-and-customer-segmentation-in-python-9d410746e122)\n",
        "- [How to Use R with Google Colab Notebook](https://medium.com/p/how-to-use-r-with-google-colab-notebook-610c3a2f0eab)"
      ],
      "metadata": {
        "id": "XS2ojbFK2dBE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1enkPaBaWqZ"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPyuSQrnaYqi"
      },
      "source": [
        "* [Sklearn Isolation Forest Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html)\n",
        "* [Isolation Forest Warm Start](https://scikit-learn.org/stable/modules/outlier_detection.html#isolation-forest)\n",
        "* [Isolation Forest Paper](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf)"
      ]
    }
  ]
}